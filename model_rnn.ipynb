{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from source import data_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense, Activation, Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439, 30, 71)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('data/feature_extract_reshaped_all.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((351, 30, 71), (88, 30, 71))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test set\n",
    "\n",
    "train_split_ratio = int((data.shape[0])*0.8)  # the first dimensio is epoch, and we are splitting the epochs between train anf test\n",
    "train_data = data[:train_split_ratio,:,:]\n",
    "test_data = data[train_split_ratio:,:,:]\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target =train_data[:,:,-1]\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 30, 70)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features =train_data[:,:,:-1]\n",
    "features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (351, 30, 70)\n",
      "y_train shape: (351, 30)\n",
      "X_test shape: (88, 30, 70)\n",
      "y_test shape: (88, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data[:,:,:-1]\n",
    "y_train = train_data[:,:,-1]\n",
    "X_test = test_data[:,:,:-1]\n",
    "y_test = test_data[:,:,-1]\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_1d shape: (351,)\n",
      "y_test_1d shape: (88,)\n"
     ]
    }
   ],
   "source": [
    "y_train_1d = y_train.max(axis=1)\n",
    "y_test_1d = y_test.max(axis=1)\n",
    "print(f\"y_train_1d shape: {y_train_1d.shape}\")\n",
    "print(f\"y_test_1d shape: {y_test_1d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions = np.isnan(y_train)\n",
    "np.any(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351, 30, 70)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 2s 21ms/step - loss: 0.3150 - accuracy: 0.0456\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 0.1866 - accuracy: 0.0114\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.1705 - accuracy: 0.0114\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.1658 - accuracy: 0.0057\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.1674 - accuracy: 0.0028\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.1630 - accuracy: 0.0085\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.1650 - accuracy: 0.0114\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1611 - accuracy: 0.0057\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1620 - accuracy: 0.0171\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1628 - accuracy: 0.0085\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1588 - accuracy: 0.0057\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1591 - accuracy: 0.0228\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1638 - accuracy: 0.0057\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1602 - accuracy: 0.0142\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1543 - accuracy: 0.0085\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1548 - accuracy: 0.0085\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1612 - accuracy: 0.0085\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1574 - accuracy: 0.0085\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1602 - accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1619 - accuracy: 0.0199\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1578 - accuracy: 0.0085\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1591 - accuracy: 0.0057\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1528 - accuracy: 0.0085\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1495 - accuracy: 0.0142\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1647 - accuracy: 0.0085\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1523 - accuracy: 0.0028\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1548 - accuracy: 0.0199\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1560 - accuracy: 0.0171\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1506 - accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1476 - accuracy: 0.0228\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1433 - accuracy: 0.0085\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1456 - accuracy: 0.0114\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1459 - accuracy: 0.0171\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1479 - accuracy: 0.0142\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1480 - accuracy: 0.0028\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1443 - accuracy: 0.0085\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1410 - accuracy: 0.0256\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1369 - accuracy: 0.0085\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1515 - accuracy: 0.0142\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1443 - accuracy: 0.0142\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1422 - accuracy: 0.0114\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1434 - accuracy: 0.0028\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1421 - accuracy: 0.0085\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1337 - accuracy: 0.0228\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1385 - accuracy: 0.0228\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1360 - accuracy: 0.0199\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1238 - accuracy: 0.0114\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1302 - accuracy: 0.0171\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1198 - accuracy: 0.0171\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.1182 - accuracy: 0.0114\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.1240 - accuracy: 0.0313\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.1251 - accuracy: 0.0171\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.1233 - accuracy: 0.0057\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.1171 - accuracy: 0.0085\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.1122 - accuracy: 0.0427\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.1148 - accuracy: 0.0171\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1119 - accuracy: 0.0313\n",
      "Epoch 58/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.1125 - accuracy: 0.0142\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.1165 - accuracy: 0.0142\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.1152 - accuracy: 0.0142\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.0342\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1162 - accuracy: 0.0057\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1159 - accuracy: 0.0142\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1174 - accuracy: 0.0228\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1200 - accuracy: 0.0114\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1145 - accuracy: 0.0085\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1101 - accuracy: 0.0114\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.0057\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0954 - accuracy: 0.0028\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1022 - accuracy: 0.0114\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1038 - accuracy: 0.0028\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1253 - accuracy: 0.0085\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1053 - accuracy: 0.0057\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0998 - accuracy: 0.0142\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1025 - accuracy: 0.0142\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1097 - accuracy: 0.0171\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1014 - accuracy: 0.0199\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0949 - accuracy: 0.0171\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0988 - accuracy: 0.0199\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0944 - accuracy: 0.0114\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1028 - accuracy: 0.0142\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1014 - accuracy: 0.0085\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1025 - accuracy: 0.0142\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0972 - accuracy: 0.0370\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.1060 - accuracy: 0.0228\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1023 - accuracy: 0.0114\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0972 - accuracy: 0.0228\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0919 - accuracy: 0.0228\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0884 - accuracy: 0.0285\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0904 - accuracy: 0.0228\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0901 - accuracy: 0.0057\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0808 - accuracy: 0.0142\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0899 - accuracy: 0.0057\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0854 - accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0942 - accuracy: 0.0285\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.0142\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1005 - accuracy: 0.0028\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0935 - accuracy: 0.0114\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0804 - accuracy: 0.0028\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1188 - accuracy: 0.0085\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.initializers import GlorotUniform\n",
    "\n",
    "initializer = GlorotUniform()  # You can choose different initializers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Input(shape=X_train.shape[1:]), kernel_initializer=initializer)\n",
    "model.add(LSTM(units=50, input_shape=X_train.shape[1:], kernel_initializer=initializer, return_sequences=False, \n",
    "               dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# # Recurrent layer\n",
    "# model.add(LSTM(64, return_sequences=False, \n",
    "#                dropout=0.1, recurrent_dropout=0.1))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.01\n",
    "adam_optimizer = Adam(learning_rate=learning_rate, clipvalue=2.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100)\n",
    "# history = model.fit(X_train, y_train_1d, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa03ff1fe20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.7863083e-04, 1.3197993e-04, 1.8590198e-04, ..., 1.3677372e-01,\n",
       "        1.4174931e-01, 1.2889439e-01],\n",
       "       [7.9300014e-05, 6.6943401e-05, 7.6219192e-05, ..., 8.2118884e-02,\n",
       "        8.4770575e-02, 7.7015303e-02],\n",
       "       [1.9917441e-04, 1.4472184e-04, 2.0759609e-04, ..., 1.4762381e-01,\n",
       "        1.5291797e-01, 1.3967624e-01],\n",
       "       ...,\n",
       "       [2.5302777e-04, 1.7230930e-04, 2.6828915e-04, ..., 1.9311816e-01,\n",
       "        2.0029886e-01, 1.8665610e-01],\n",
       "       [6.7393562e-06, 7.2500316e-06, 8.1746757e-06, ..., 4.8165027e-02,\n",
       "        4.6492904e-02, 5.3448688e-02],\n",
       "       [3.7300380e-04, 2.0142183e-04, 4.1773627e-04, ..., 4.6721911e-01,\n",
       "        4.5345420e-01, 4.6927902e-01]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 0.1839 - accuracy: 0.0000e+00\n",
      "Loss: 0.1839415580034256\n",
      "Accuracy: 0.0\n",
      "Precision: 0.145\n",
      "Recall: 0.07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "y_pred_c = y_pred.round().astype(int)\n",
    "precision = precision_score(y_test, y_pred_c, average='macro')\n",
    "recall = recall_score(y_test, y_pred_c, average='macro')\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# # Create callbacks\n",
    "# callbacks = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_model(input_shape):\n",
    "#     input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "#     conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "#     conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "#     conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "#     conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "#     conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "#     conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "#     conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "#     conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "#     conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "#     gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "#     output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "#     return keras.models.Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = make_model(input_shape=X_train.shape[1:])\n",
    "# keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 500\n",
    "# batch_size = 32\n",
    "\n",
    "# callbacks = [\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         \"best_model.h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "#     ),\n",
    "#     keras.callbacks.ReduceLROnPlateau(\n",
    "#         monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "#     ),\n",
    "#     keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "# ]\n",
    "# model.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     metrics=[\"sparse_categorical_accuracy\"],\n",
    "# )\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=callbacks,\n",
    "#     #validation_split=0.2,\n",
    "#     verbose=1,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs: A 3D tensor with shape [batch, timesteps, feature]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a sequential model\n",
    "# model = keras.Sequential()\n",
    "\n",
    "# # Add an input layer with an input shape of (time_steps, input_features)\n",
    "# model.add(layers.InputLayer(input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "\n",
    "# # Add a SimpleRNN layer with 32 units (neurons)\n",
    "# model.add(layers.SimpleRNN(32, activation='tanh'))\n",
    "\n",
    "# # Add the output layer with 1 neuron and sigmoid activation for binary classification\n",
    "# model.add(layers.Dense(30, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model with binary cross-entropy loss for binary classification\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are your training data\n",
    "# model.fit(X_train, y_train, epochs=100, batch_size=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
