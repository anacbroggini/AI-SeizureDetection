{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "source_dir = Path.cwd().parent / 'source'\n",
    "sys.path.append(str(source_dir))\n",
    "\n",
    "import data_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weasel/reps/ai-seizure-detectives/source/../data/segmented_data_300.arrow was loaded.\n",
      "/home/weasel/reps/ai-seizure-detectives/source/../data/deepheaven2saizure_pure.arrow was successfully written.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4377600, 12), (4377600, 12))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'deepheaven2saizure_pure'\n",
    "\n",
    "data = data_import.load_pyarrow(file_name='segmented_data_300')\n",
    "data.head()\n",
    "\n",
    "data = data.drop(columns=['is_seizure', 'before_seizure', 'file', 'segment_id', 'seizure_start'])\n",
    "data['target'] = (data['target'] > pd.Timedelta('0s')).astype(int)\n",
    "data_import.save_pyarrow(data, file_name=file_name)\n",
    "data_1 = data[data['target'] == 1].copy()\n",
    "data_0 = data[data['target'] == 0].copy()\n",
    "data_0 = data_0.iloc[:data_1.shape[0], :]\n",
    "data_0.shape, data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4377600, 11), (4377600, 11))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(data_1['target'].sum())\n",
    "# print(data_0['target'].sum())\n",
    "\n",
    "data_0 = data_0.drop(columns=['target'])\n",
    "data_1 = data_1.drop(columns=['target'])\n",
    "\n",
    "# D = data.values\n",
    "# np.save('data/'+ file_name + '.npy', D)\n",
    "# D = D.reshape(data['epoch'].nunique(), -1)\n",
    "data_0.shape, data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Construct LSTM sequences from one segment\n",
    "def lstm_sequence(input_segment, target, sampling_freq, window, stride, block_s = 60):\n",
    "    \"\"\" Function for generating blocks of LSTM input tensors\n",
    "        input_segment : The EEG segment\n",
    "        target        : 1/0 (preictal/interictial); None for test\n",
    "        sampling_freq : Samplig frequency\n",
    "        window        : Window size for 1d convolutions on each block\n",
    "        stride        : Stride size of the 1d convolution\n",
    "        block_s       : Size of the block in seconds (default = 60)\n",
    "    \"\"\"\n",
    "\n",
    "    # Dimensions\n",
    "    n_channels, T_segment = input_segment.shape\n",
    "\n",
    "    # Determine block dimensions\n",
    "    block_len = sampling_freq * block_s   # Length of each block\n",
    "    n_blocks = (T_segment-1) // block_len # Number of blocks\n",
    "    blocks = [block for block in range(0,(n_blocks+1)*block_len,block_len)]\n",
    "\n",
    "    # Determine the sequence length for LSTM\n",
    "    div = (block_len - window)%stride\n",
    "    if (div != 0):\n",
    "        pad = stride - div # Size of padding neded\n",
    "    else:\n",
    "        pad = 0\n",
    "\n",
    "    seq_len = (block_len + pad - window) // stride\n",
    "\n",
    "    # Initiate tensor\n",
    "    X = np.zeros((n_blocks, seq_len, n_channels))\n",
    "\n",
    "    # Loop over blocks and fill X\n",
    "    for ib in range(n_blocks):\n",
    "        # Get block\n",
    "        data_block = input_segment[:, blocks[ib]:blocks[ib+1]]\n",
    "\n",
    "        # Pad if necessary\n",
    "        if (pad !=0):\n",
    "            data_block = np.concatenate((data_block, np.zeros((n_channels, pad))), axis=1)\n",
    "\n",
    "        # 1d convolution by mean\n",
    "        index = 0\n",
    "        for j in range(seq_len):\n",
    "            X[ib, j, :] = np.mean(data_block[:, (index+j):(index+j+seq_len)], axis = 1)\n",
    "\n",
    "    # Fill in the target\n",
    "    if (target == 1):\n",
    "        Y = np.ones(n_blocks)\n",
    "    elif(target == 0):\n",
    "        Y = np.zeros(n_blocks)\n",
    "    else:\n",
    "        Y = None\n",
    "\n",
    "    return X, Y, n_blocks\n",
    "\n",
    "\n",
    "# Collect all the segments to build a tesnsor input for LSTM\n",
    "def lstm_build_input(df, target, window, stride, block_s = 60):\n",
    "    \"\"\" Collect all the data and build sequences for LSTM\n",
    "        clips              : List of clips\n",
    "        target             : 1/0 (preictal/interictial); None for test set\n",
    "        window             : Window size for 1d convolutions\n",
    "        stride             : Length of the stride in 1d convolution\n",
    "        block_s            : Size of the block in seconds (default = 60)\n",
    "    \"\"\"\n",
    "    epochs = [df[df['epoch'] == epoch] for epoch in df['epoch'].unique()]\n",
    "    # Number of clips\n",
    "    n_epochs = len(epochs)\n",
    "\n",
    "    # Loop over all clips and store data\n",
    "    iepoch = 0\n",
    "    for epoch in epochs:\n",
    "        # segment_name = list(clip.keys())[3] # Get segment name\n",
    "\n",
    "        input_segment = epoch.drop(columns=['epoch']).values.T\n",
    "        sampling_freq = 256\n",
    "\n",
    "        # Get number of channels\n",
    "        n_channels = epoch.shape[1] - 2\n",
    "\n",
    "        # Get tensor input and targets from blocks\n",
    "        X, Y, n_blocks = lstm_sequence(input_segment, target, sampling_freq, window, stride, block_s)\n",
    "\n",
    "        # Concatenate the tensor and target vector\n",
    "        if (iepoch == 0):\n",
    "            X_train = X\n",
    "            Y_train = Y[:,None] if Y is not None else None\n",
    "        else:\n",
    "            X_train = np.vstack((X_train,X))\n",
    "            Y_train = np.vstack((Y_train,Y[:,None])) if Y is not None else None\n",
    "\n",
    "        iepoch +=1\n",
    "\n",
    "    return X_train, Y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape =  (456, 2816, 10)\n"
     ]
    }
   ],
   "source": [
    "# Window, stride and block_s\n",
    "# window = 16000\n",
    "# stride = 100\n",
    "# block_s = 60\n",
    "window = 5 * 256\n",
    "stride = 5\n",
    "block_s = 60\n",
    "\n",
    "X_1, Y_1 = lstm_build_input(data_1, 1, window, stride)\n",
    "X_0, Y_0 = lstm_build_input(data_0, 0, window, stride)\n",
    "\n",
    "# Scale the data\n",
    "X_1 = X_1 / np.max(np.abs(X_1), axis=1)[:,None,:]\n",
    "X_0 = X_0 / np.max(np.abs(X_0), axis=1)[:,None,:]\n",
    "\n",
    "# Combine the data\n",
    "X = np.concatenate((X_0, X_1), axis = 0)\n",
    "Y = np.concatenate((Y_0, Y_1), axis = 0)\n",
    "Y = np.squeeze(Y)\n",
    "n_channels = X.shape[2]\n",
    "\n",
    "print(\"Data shape = \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((456, 2816, 10), (456,), 228.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape, Y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "X = X / np.max(np.abs(X), axis=1)[:,None,:]\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(1)\n",
    "shuffle = np.random.choice(np.arange(len(Y)), size=len(Y), replace=False)\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 16:08:17.511491: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 16:08:17.660403: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-10 16:08:17.660427: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-10 16:08:17.661141: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-10 16:08:17.738227: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 16:08:17.739395: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-10 16:08:18.814949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Input(shape=(X.shape[1], X.shape[2])))\n",
    "model.add(layers.LSTM(64))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y_class[index]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# X_table = X_table.update([\"Class = (int)add_class_col(i)\"])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m train_model(X_train, Y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# # Train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# learn.learn(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m#     table=X_table,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m# Convert numpy array X_test to DH table X_table_test\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m X_reshaped_test \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39mreshape(X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], n_cols)\n",
      "\u001b[1;32m/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m roc \u001b[39m=\u001b[39m RocCallback(training_data\u001b[39m=\u001b[39m(X_train, Y_train), validation_data\u001b[39m=\u001b[39m(X_valid, Y_valid))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bdebian/home/weasel/reps/ai-seizure-detectives/.temp/deepheaven2saizure_pure.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, Y_train, validation_data\u001b[39m=\u001b[39;49m(X_valid, Y_valid), callbacks\u001b[39m=\u001b[39;49m[roc], batch_size \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6b6d_kt7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1131, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1225, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/home/weasel/reps/ai-seizure-detectives/.venv/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class RocCallback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_train = model.predict(self.x)\n",
    "        roc_train = roc_auc_score(self.y, y_pred_train)\n",
    "        y_pred_val = model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('roc-auc_train: ', roc_train)\n",
    "        print('roc-auc_val: ', roc_val)\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "# Function that trains the model\n",
    "def train_model(X, Y):\n",
    "    X = X.reshape(X.shape[0], -1, n_channels)   # reshape DH table to 3d numpy array\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, stratify=Y, test_size = 0.1)\n",
    "    roc = RocCallback(training_data=(X_train, Y_train), validation_data=(X_valid, Y_valid))\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=[\"recall\"])\n",
    "    \n",
    "    model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), callbacks=[roc], batch_size = 200, epochs=100)\n",
    "\n",
    "    # from sklearn.utils import class_weight\n",
    "    # class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(Y), y=Y)\n",
    "    # model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), callbacks=[roc], batch_size = 200, epochs=100, class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "# Function that gets the model's predictions on input data\n",
    "def predict_with_model(X):\n",
    "    X = X.reshape(X.shape[0], -1, n_channels)  # reshape DH table to 3d numpy array\n",
    "    Y_pred = model.predict(X, batch_size=200)\n",
    "    return Y_pred\n",
    "\n",
    "# Function to extract a list element at a given index\n",
    "def get_predicted_class(data, idx):\n",
    "    return data[idx]\n",
    "\n",
    "# Split the data into training and test datasets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, test_size = 0.2)\n",
    "\n",
    "# Convert numpy arrays X_train and Y_train to DH table X_table\n",
    "n_rows = X_train.shape[0]\n",
    "n_cols = X_train.shape[1] * X_train.shape[2]\n",
    "column_names = ['Col_'+str(i) for i in range(n_cols)]\n",
    "X_reshaped = X_train.reshape(n_rows, n_cols)\n",
    "# X_table = numpy.to_table(X_reshaped, cols=column_names)\n",
    "\n",
    "def add_class_col(index):\n",
    "    y_class = [int(i) for i in Y_train.tolist()]\n",
    "    return y_class[index]\n",
    "\n",
    "# X_table = X_table.update([\"Class = (int)add_class_col(i)\"])\n",
    "\n",
    "\n",
    "train_model(X_train, Y_train)\n",
    "\n",
    "# # Train the model\n",
    "# learn.learn(\n",
    "#     table=X_table,\n",
    "#     model_func=train_model,\n",
    "#     inputs=[learn.Input(column_names, table_to_array_double), learn.Input([\"Class\"], table_to_array_int)],\n",
    "#     outputs=None,\n",
    "#     batch_size=200\n",
    "# )\n",
    "\n",
    "\n",
    "# Convert numpy array X_test to DH table X_table_test\n",
    "X_reshaped_test = X_test.reshape(X_test.shape[0], n_cols)\n",
    "# X_table_test = numpy.to_table(X_reshaped_test, cols=column_names)\n",
    "\n",
    "# Use the learn function to create a new table that contains predicted values\n",
    "y_pred = predict_with_model(X_test)\n",
    "\n",
    "# predicted = learn.learn(\n",
    "#     table=X_table_test,\n",
    "#     model_func=predict_with_model,\n",
    "#     inputs=[learn.Input(column_names, table_to_array_double)],\n",
    "#     outputs=[learn.Output(\"PredictedClass\", get_predicted_class, \"int\")],\n",
    "#     batch_size=200\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 240ms/step\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 1.1477 - accuracy: 0.5109\n",
      "Loss: 1.1476995944976807\n",
      "Accuracy: 0.510869562625885\n",
      "Precision: 0.5111111111111111\n",
      "Recall: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46.0, 45)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_with_model(X_test)\n",
    "y_test = Y_test\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "y_pred_c = y_pred.round().astype(int)\n",
    "precision = precision_score(y_test, y_pred_c)\n",
    "recall = recall_score(y_test, y_pred_c)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "Y_test.sum(), y_pred_c.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 46.0, 23)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_c.sum(), y_test.sum(), (y_pred_c[:,0].astype(bool) & y_test.astype(bool)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 355ms/step\n",
      "4/4 [==============================] - 1s 135ms/step - loss: 0.8821 - accuracy: 0.7398\n",
      "Loss: 0.8820559978485107\n",
      "Accuracy: 0.7398374080657959\n",
      "Precision: 0.5454545454545454\n",
      "Recall: 0.18181818181818182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33.0, 11)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_with_model(X_test)\n",
    "y_test = Y_test\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "y_pred_c = y_pred.round().astype(int)\n",
    "precision = precision_score(y_test, y_pred_c)\n",
    "recall = recall_score(y_test, y_pred_c)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "Y_test.sum(), y_pred_c.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
